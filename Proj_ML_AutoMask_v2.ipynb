{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Based Automatic Mask Generation for Pyramid Blending\n",
    "## With Interactive File Upload\n",
    "\n",
    "This notebook uses MediaPipe for automatic face detection and mask generation,\n",
    "combined with the pyramid blending algorithm from Proj_opt1.ipynb.\n",
    "\n",
    "**Upload your own images directly in the notebook!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        # For mediapipe, check if we need Colab-compatible version\n",
    "        if package.startswith('mediapipe'):\n",
    "            import mediapipe as mp\n",
    "            print(f\"âœ… {package} already installed (v{mp.__version__})\")\n",
    "        else:\n",
    "            __import__(package.split('[')[0])\n",
    "            print(f\"âœ… {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"ðŸ“¦ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"âœ… {package} installed\")\n",
    "\n",
    "# For Google Colab, use mediapipe>=0.10.14\n",
    "# For local, any version works\n",
    "try:\n",
    "    from google.colab import files\n",
    "    # Running on Colab - install specific version\n",
    "    print(\"ðŸ” Running on Google Colab - installing Colab-compatible versions...\\n\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"mediapipe>=0.10.14\"])\n",
    "    print(\"âœ… MediaPipe (Colab version) installed\")\n",
    "except ImportError:\n",
    "    # Not on Colab - install normally\n",
    "    install_package(\"mediapipe\")\n",
    "\n",
    "install_package(\"opencv-python\")\n",
    "install_package(\"numpy\")\n",
    "install_package(\"matplotlib\")\n",
    "install_package(\"pillow\")\n",
    "install_package(\"ipywidgets\")\n",
    "\n",
    "print(\"\\nâœ… All packages ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import io\n",
    "\n",
    "# Try to import Google Colab files module\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "    COLAB_AVAILABLE = True\n",
    "    print(\"âœ… Running on Google Colab\")\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "    print(\"âœ… Running on local Jupyter\")\n",
    "\n",
    "print(f\"âœ… NumPy {np.__version__}\")\n",
    "print(f\"âœ… OpenCV {cv2.__version__}\")\n",
    "print(f\"âœ… MediaPipe {mp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions from Proj_opt1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def pad_image(image, kernel, pad_type):\n    \"\"\"Pad image for convolution (from Proj_opt1.ipynb).\"\"\"\n    height, width = kernel.shape\n    pad_height = height // 2\n    pad_width = width // 2\n    \n    if pad_type == 'clip':\n        padded_image = np.pad(image, ((pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='constant')\n    elif pad_type == 'wrap':\n        padded_image = np.pad(image, ((pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='wrap')\n    elif pad_type == 'copy_edge':\n        padded_image = np.pad(image, ((pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='edge')\n    elif pad_type == 'reflect':\n        padded_image = np.pad(image, ((pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='reflect')\n    else:\n        raise ValueError(\"Invalid padding type.\")\n    \n    return padded_image\n\ndef conv2(image, kernel, pad_type):\n    \"\"\"Perform 2D convolution using matrix slicing (from Proj_opt1.ipynb).\"\"\"\n    assert len(kernel.shape) == 2, \"Kernel must be 2D.\"\n    \n    if len(image.shape) == 2:\n        image = image[:, :, np.newaxis]\n    \n    height, width, channels = image.shape\n    k_height, k_width = kernel.shape\n    \n    padded_image = pad_image(image, kernel, pad_type)\n    output = np.zeros((height, width, channels))\n    \n    flipped_kernel = kernel[::-1, ::-1]\n    \n    for c in range(channels):\n        windows = np.lib.stride_tricks.sliding_window_view(\n            padded_image[:, :, c], (k_height, k_width)\n        )\n        output[:, :, c] = np.tensordot(windows, flipped_kernel, axes=((2, 3), (0, 1)))\n    \n    if output.shape[2] == 1:\n        output = output[:, :, 0]\n    \n    return output\n\ndef downsample(image):\n    \"\"\"Downsample by factor of 2.\"\"\"\n    return image[::2, ::2]\n\ndef upsample(image, target_size):\n    \"\"\"Upsample with smoothing.\"\"\"\n    if len(image.shape) == 3:\n        upsampled = np.zeros((target_size[0], target_size[1], image.shape[2]), dtype=image.dtype)\n        upsampled[::2, ::2, :] = image\n    else:\n        upsampled = np.zeros(target_size, dtype=image.dtype)\n        upsampled[::2, ::2] = image\n    \n    upsampled = cv2.GaussianBlur(upsampled, (5, 5), 0)\n    return upsampled\n\ndef ComputePyr(input_image, num_layers):\n    \"\"\"Compute Gaussian and Laplacian pyramids (from Proj_opt1.ipynb).\"\"\"\n    max_layers = int(np.floor(np.log2(min(input_image.shape[:2])))) - 1\n    num_layers = min(num_layers, max_layers)\n    \n    kernel = cv2.getGaussianKernel(5, 1)\n    kernel = np.outer(kernel, kernel)\n    \n    gPyr = [input_image.astype(np.float32)]\n    lPyr = []\n    \n    for i in range(num_layers):\n        smoothed = conv2(gPyr[-1], kernel, 'wrap')\n        downsampled = downsample(smoothed)\n        gPyr.append(downsampled)\n        \n        target_size = (gPyr[-2].shape[0], gPyr[-2].shape[1])\n        upsampled = upsample(downsampled, target_size)\n        laplacian = gPyr[-2] - upsampled\n        lPyr.append(laplacian)\n    \n    lPyr.append(gPyr[-1])\n    return gPyr, lPyr\n\ndef create_smooth_edge_mask(mask, edge_width=80, num_passes=3):\n    \"\"\"\n    Create ultra-smooth edge transitions using multi-pass gaussian blur.\n    \n    Args:\n        mask: Binary mask (0-255)\n        edge_width: Width of the smooth edge region in pixels\n        num_passes: Number of gaussian blur passes for smoother edges\n    \n    Returns:\n        Smoothed mask with gradual falloff at edges\n    \"\"\"\n    mask_float = mask.astype(float) / 255.0\n    \n    # Multi-pass gaussian blur for extremely smooth edges\n    for i in range(num_passes):\n        # Progressively larger kernels\n        kernel_size = edge_width // (num_passes - i) if i < num_passes - 1 else edge_width\n        kernel_size = kernel_size if kernel_size % 2 == 1 else kernel_size + 1\n        mask_float = cv2.GaussianBlur(mask_float, (kernel_size, kernel_size), 0)\n    \n    # Apply distance transform for natural falloff\n    mask_binary = (mask > 127).astype(np.uint8)\n    dist_transform = cv2.distanceTransform(mask_binary, cv2.DIST_L2, 5)\n    \n    # Normalize distance transform\n    if dist_transform.max() > 0:\n        dist_transform = dist_transform / dist_transform.max()\n    \n    # Combine gaussian blur with distance transform for optimal smoothness\n    # Use sigmoidal function for smooth transition\n    dist_weight = np.clip(dist_transform * 2, 0, 1)\n    smooth_mask = mask_float * 0.7 + dist_weight * 0.3\n    \n    return np.clip(smooth_mask, 0, 1)\n\ndef match_color_histogram(source, target, mask):\n    \"\"\"\n    Advanced color matching using histogram matching in LAB color space.\n    \n    Args:\n        source: Source image (RGB)\n        target: Target image (RGB)\n        mask: Mask indicating region to match (0-1 float)\n    \n    Returns:\n        Color-matched source image\n    \"\"\"\n    # Convert to LAB color space for better color matching\n    source_lab = cv2.cvtColor(source.astype(np.uint8), cv2.COLOR_RGB2LAB).astype(float)\n    target_lab = cv2.cvtColor(target.astype(np.uint8), cv2.COLOR_RGB2LAB).astype(float)\n    \n    # Match mean and std for each channel in masked region\n    mask_3d = mask[..., None] if len(mask.shape) == 2 else mask\n    \n    matched_lab = source_lab.copy()\n    for c in range(3):\n        source_channel = source_lab[:, :, c]\n        target_channel = target_lab[:, :, c]\n        \n        # Calculate statistics in masked region\n        mask_area = np.sum(mask_3d > 0.1)\n        if mask_area > 0:\n            source_mean = np.sum(source_channel * mask_3d[:, :, 0]) / mask_area\n            source_std = np.sqrt(np.sum((source_channel - source_mean)**2 * mask_3d[:, :, 0]) / mask_area)\n            \n            target_mean = np.sum(target_channel * mask_3d[:, :, 0]) / mask_area\n            target_std = np.sqrt(np.sum((target_channel - target_mean)**2 * mask_3d[:, :, 0]) / mask_area)\n            \n            # Match histogram\n            if source_std > 1e-5:\n                matched_lab[:, :, c] = (source_channel - source_mean) * (target_std / source_std) + target_mean\n    \n    # Convert back to RGB\n    matched_lab = np.clip(matched_lab, 0, 255).astype(np.uint8)\n    matched_rgb = cv2.cvtColor(matched_lab, cv2.COLOR_LAB2RGB)\n    \n    return matched_rgb\n\ndef blend_with_pyramids(target_image, clipped_image, mask, levels=6):\n    \"\"\"\n    OPTIMIZED: Blend using Laplacian pyramids with enhanced edge smoothness.\n    \n    Improvements:\n    - Multi-pass gaussian blur for ultra-smooth mask edges\n    - Advanced color histogram matching in LAB space\n    - Progressive mask feathering across pyramid levels\n    - Edge-aware blending to preserve facial features\n    \"\"\"\n    clipped_image = cv2.resize(clipped_image, (target_image.shape[1], target_image.shape[0]))\n    mask = cv2.resize(mask, (target_image.shape[1], target_image.shape[0]))\n    \n    # Step 1: Create ultra-smooth edge mask\n    print(\"   Preprocessing mask for smooth edges...\")\n    mask_normalized = mask / 255.0\n    smoothed_mask = create_smooth_edge_mask(mask, edge_width=100, num_passes=4)\n    \n    # Step 2: Advanced color matching in LAB space\n    print(\"   Performing advanced color matching...\")\n    clipped_adjusted = match_color_histogram(clipped_image, target_image, smoothed_mask)\n    \n    # Step 3: Build pyramids\n    print(\"   Building image pyramids...\")\n    gPyr_target, lPyr_target = ComputePyr(target_image, levels)\n    gPyr_clipped, lPyr_clipped = ComputePyr(clipped_adjusted, levels)\n    gPyr_mask, _ = ComputePyr(smoothed_mask, levels)\n    \n    # Step 4: Multi-scale blending with progressive feathering\n    print(\"   Blending across pyramid levels...\")\n    lPyr_blended = []\n    for level, (l_target, l_clipped, g_mask) in enumerate(zip(lPyr_target, lPyr_clipped, gPyr_mask)):\n        # Apply additional smoothing to mask at each level for better transitions\n        level_mask = g_mask\n        if level < len(gPyr_mask) - 1:\n            kernel_size = 15 - level * 2\n            kernel_size = max(3, kernel_size if kernel_size % 2 == 1 else kernel_size + 1)\n            level_mask = cv2.GaussianBlur(g_mask, (kernel_size, kernel_size), 0)\n        \n        # Blend with progressive mask\n        if len(level_mask.shape) == 2:\n            level_mask = level_mask[..., None]\n        \n        blended = l_target * (1 - level_mask) + l_clipped * level_mask\n        lPyr_blended.append(blended)\n    \n    # Step 5: Reconstruct from pyramid\n    print(\"   Reconstructing final image...\")\n    blended_image = lPyr_blended[-1]\n    for i in range(len(lPyr_blended) - 2, -1, -1):\n        blended_image = upsample(blended_image, lPyr_blended[i].shape[:2])\n        blended_image = cv2.add(blended_image, lPyr_blended[i])\n    \n    # Step 6: Final blend with ultra-smooth mask\n    smoothed_mask_3d = smoothed_mask[..., None] if len(smoothed_mask.shape) == 2 else smoothed_mask\n    final_result = target_image * (1 - smoothed_mask_3d) + blended_image * smoothed_mask_3d\n    \n    # Step 7: Apply subtle post-processing for seamless edges\n    print(\"   Applying edge refinement...\")\n    final_result = np.clip(final_result, 0, 255).astype(np.uint8)\n    \n    # Bilateral filter only in edge region for edge-aware smoothing\n    edge_region = (smoothed_mask > 0.1) & (smoothed_mask < 0.9)\n    if edge_region.any():\n        filtered = cv2.bilateralFilter(final_result, d=9, sigmaColor=75, sigmaSpace=75)\n        edge_blend = edge_region.astype(float) * (1 - np.abs(smoothed_mask - 0.5) * 2)\n        edge_blend = edge_blend[..., None] if len(edge_blend.shape) == 2 else edge_blend\n        final_result = (final_result * (1 - edge_blend) + filtered * edge_blend).astype(np.uint8)\n    \n    return final_result\n\nprint(\"âœ… OPTIMIZED pyramid blending functions loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Powered Face Detection and Mask Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class FaceDetector:\n    \"\"\"AI-powered face detection using MediaPipe.\"\"\"\n    \n    # Face oval landmark indices for mask generation\n    FACE_OVAL_INDICES = [\n        10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,\n        397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,\n        172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109\n    ]\n    \n    # Key facial landmarks\n    LEFT_EYE_CENTER = 33\n    RIGHT_EYE_CENTER = 263\n    NOSE_TIP = 1\n    MOUTH_CENTER = 13\n    \n    def __init__(self):\n        \"\"\"Initialize MediaPipe Face Mesh - supports both old and new API.\"\"\"\n        import mediapipe as mp\n        \n        # Try new API first (mediapipe >= 0.10.8)\n        try:\n            from mediapipe.tasks import python\n            from mediapipe.tasks.python import vision\n            import os\n            import urllib.request\n            \n            # Download model if not exists\n            model_path = 'face_landmarker.task'\n            if not os.path.exists(model_path):\n                print(\"   Downloading face landmarker model...\")\n                model_url = 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task'\n                urllib.request.urlretrieve(model_url, model_path)\n                print(\"   Model downloaded!\")\n            \n            base_options = python.BaseOptions(model_asset_path=model_path)\n            options = vision.FaceLandmarkerOptions(\n                base_options=base_options,\n                running_mode=vision.RunningMode.IMAGE,\n                num_faces=1,\n                min_face_detection_confidence=0.5,\n                min_face_presence_confidence=0.5,\n                min_tracking_confidence=0.5\n            )\n            self.face_mesh = vision.FaceLandmarker.create_from_options(options)\n            self.use_new_api = True\n            print(\"   Using new MediaPipe API (tasks.vision)\")\n            \n        except Exception as e:\n            # Fallback to old API (mediapipe < 0.10.8)\n            try:\n                self.face_mesh = mp.solutions.face_mesh.FaceMesh(\n                    static_image_mode=True,\n                    max_num_faces=1,\n                    refine_landmarks=True,\n                    min_detection_confidence=0.5\n                )\n                self.use_new_api = False\n                print(\"   Using legacy MediaPipe API (solutions)\")\n            except AttributeError:\n                raise ImportError(\n                    f\"Could not initialize MediaPipe Face Mesh. Error: {e}\"\n                )\n    \n    def detect_face(self, image):\n        \"\"\"Detect face and extract 468 landmarks.\"\"\"\n        h, w = image.shape[:2]\n        \n        if self.use_new_api:\n            # New API\n            import mediapipe as mp\n            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n            results = self.face_mesh.detect(mp_image)\n            \n            if not results.face_landmarks:\n                return None\n            \n            landmarks = results.face_landmarks[0]\n            landmark_points = []\n            for lm in landmarks:\n                x = int(lm.x * w)\n                y = int(lm.y * h)\n                landmark_points.append((x, y))\n        else:\n            # Old API\n            results = self.face_mesh.process(image)\n            \n            if not results.multi_face_landmarks:\n                return None\n            \n            landmarks = results.multi_face_landmarks[0]\n            landmark_points = []\n            for lm in landmarks.landmark:\n                x = int(lm.x * w)\n                y = int(lm.y * h)\n                landmark_points.append((x, y))\n        \n        key_points = {\n            'left_eye': np.array(landmark_points[self.LEFT_EYE_CENTER]),\n            'right_eye': np.array(landmark_points[self.RIGHT_EYE_CENTER]),\n            'nose': np.array(landmark_points[self.NOSE_TIP]),\n            'mouth': np.array(landmark_points[self.MOUTH_CENTER])\n        }\n        \n        return {\n            'landmarks': landmark_points,\n            'key_points': key_points\n        }\n    \n    def create_face_mask(self, image, face_data, expand_ratio=0.18):\n        \"\"\"Create mask from face oval landmarks.\"\"\"\n        h, w = image.shape[:2]\n        mask = np.zeros((h, w), dtype=np.uint8)\n        \n        # Get face oval points\n        oval_points = [face_data['landmarks'][i] for i in self.FACE_OVAL_INDICES]\n        oval_points = np.array(oval_points, dtype=np.int32)\n        \n        # Expand contour for better coverage\n        center = np.mean(oval_points, axis=0)\n        expanded_points = center + (oval_points - center) * (1 + expand_ratio)\n        expanded_points = expanded_points.astype(np.int32)\n        \n        # Fill mask\n        cv2.fillPoly(mask, [expanded_points], 255)\n        \n        # Apply light smoothing\n        mask_float = mask.astype(float) / 255.0\n        mask_float = cv2.GaussianBlur(mask_float, (15, 15), 0)\n        mask_float = np.clip(mask_float, 0, 1)\n        \n        return (mask_float * 255).astype(np.uint8)\n\nprint(\"âœ… FaceDetector class loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceAligner:\n",
    "    \"\"\"Align faces using feature-based transformation.\"\"\"\n",
    "    \n",
    "    def compute_alignment_transform(self, source_key_points, target_key_points):\n",
    "        \"\"\"Compute scale and rotation from eye positions.\"\"\"\n",
    "        src_left = source_key_points['left_eye']\n",
    "        src_right = source_key_points['right_eye']\n",
    "        tgt_left = target_key_points['left_eye']\n",
    "        tgt_right = target_key_points['right_eye']\n",
    "        \n",
    "        # Calculate scale from eye distance\n",
    "        src_eye_dist = np.linalg.norm(src_right - src_left)\n",
    "        tgt_eye_dist = np.linalg.norm(tgt_right - tgt_left)\n",
    "        scale = tgt_eye_dist / src_eye_dist\n",
    "        \n",
    "        # Calculate rotation from eye angle\n",
    "        src_angle = np.arctan2(src_right[1] - src_left[1], src_right[0] - src_left[0])\n",
    "        tgt_angle = np.arctan2(tgt_right[1] - tgt_left[1], tgt_right[0] - tgt_left[0])\n",
    "        rotation = tgt_angle - src_angle\n",
    "        \n",
    "        # Compute centers\n",
    "        src_center = (src_left + src_right) / 2\n",
    "        tgt_center = (tgt_left + tgt_right) / 2\n",
    "        \n",
    "        return {\n",
    "            'scale': scale,\n",
    "            'rotation': rotation,\n",
    "            'src_center': src_center,\n",
    "            'tgt_center': tgt_center\n",
    "        }\n",
    "    \n",
    "    def apply_alignment(self, image, transform):\n",
    "        \"\"\"Apply scale and rotation transformation.\"\"\"\n",
    "        center = (float(transform['src_center'][0]), float(transform['src_center'][1]))\n",
    "        angle = float(np.degrees(transform['rotation']))\n",
    "        scale = float(transform['scale'])\n",
    "        \n",
    "        M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "        aligned = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
    "        \n",
    "        return aligned\n",
    "    \n",
    "    def place_on_canvas(self, aligned_image, target_shape, tgt_center, src_center):\n",
    "        \"\"\"Place aligned image on canvas matching target size.\"\"\"\n",
    "        canvas = np.zeros(target_shape, dtype=aligned_image.dtype)\n",
    "        \n",
    "        offset = (tgt_center - src_center).astype(int)\n",
    "        \n",
    "        y_start = max(0, offset[1])\n",
    "        y_end = min(canvas.shape[0], offset[1] + aligned_image.shape[0])\n",
    "        x_start = max(0, offset[0])\n",
    "        x_end = min(canvas.shape[1], offset[0] + aligned_image.shape[1])\n",
    "        \n",
    "        src_y_start = max(0, -offset[1])\n",
    "        src_y_end = src_y_start + (y_end - y_start)\n",
    "        src_x_start = max(0, -offset[0])\n",
    "        src_x_end = src_x_start + (x_end - x_start)\n",
    "        \n",
    "        canvas[y_start:y_end, x_start:x_end] = aligned_image[src_y_start:src_y_end, src_x_start:src_x_end]\n",
    "        \n",
    "        return canvas\n",
    "\n",
    "print(\"âœ… FaceAligner class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Face Swap Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_face_swap(source_image, target_image, visualize=True, pyramid_levels=6):\n",
    "    \"\"\"\n",
    "    Automatic face swapping with AI detection.\n",
    "    \n",
    "    Args:\n",
    "        source_image: RGB image array (face to swap IN)\n",
    "        target_image: RGB image array (face to swap ONTO)\n",
    "        visualize: Whether to display results\n",
    "        pyramid_levels: Number of pyramid levels for blending\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ¤– AI-Powered Face Swap\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(f\"ðŸ“Š Image sizes:\")\n",
    "    print(f\"   Source: {source_image.shape}\")\n",
    "    print(f\"   Target: {target_image.shape}\")\n",
    "    \n",
    "    # Step 1: Detect faces\n",
    "    print(\"\\nðŸ” Step 1: Detecting faces...\")\n",
    "    detector = FaceDetector()\n",
    "    \n",
    "    source_face = detector.detect_face(source_image)\n",
    "    if source_face is None:\n",
    "        print(\"âŒ ERROR: No face detected in source image!\")\n",
    "        return None\n",
    "    \n",
    "    target_face = detector.detect_face(target_image)\n",
    "    if target_face is None:\n",
    "        print(\"âŒ ERROR: No face detected in target image!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   âœ… Source: {len(source_face['landmarks'])} landmarks\")\n",
    "    print(f\"   âœ… Target: {len(target_face['landmarks'])} landmarks\")\n",
    "    \n",
    "    # Step 2: Align faces\n",
    "    print(\"\\nðŸ“ Step 2: Aligning facial features...\")\n",
    "    aligner = FaceAligner()\n",
    "    \n",
    "    transform = aligner.compute_alignment_transform(\n",
    "        source_face['key_points'],\n",
    "        target_face['key_points']\n",
    "    )\n",
    "    \n",
    "    print(f\"   Scale factor: {transform['scale']:.2f}\")\n",
    "    print(f\"   Rotation: {np.degrees(transform['rotation']):.1f}Â°\")\n",
    "    \n",
    "    aligned_source = aligner.apply_alignment(source_image, transform)\n",
    "    source_canvas = aligner.place_on_canvas(\n",
    "        aligned_source,\n",
    "        target_image.shape,\n",
    "        transform['tgt_center'],\n",
    "        transform['src_center']\n",
    "    )\n",
    "    \n",
    "    # Step 3: Generate mask\n",
    "    print(\"\\nðŸŽ­ Step 3: Generating automatic face mask...\")\n",
    "    canvas_face = detector.detect_face(source_canvas)\n",
    "    if canvas_face:\n",
    "        mask = detector.create_face_mask(source_canvas, canvas_face, expand_ratio=0.15)\n",
    "        print(\"   âœ… Mask generated from aligned face contours\")\n",
    "    else:\n",
    "        mask = detector.create_face_mask(target_image, target_face, expand_ratio=0.15)\n",
    "        print(\"   âš ï¸  Using target face for mask (fallback)\")\n",
    "    \n",
    "    # Step 4: Pyramid blending\n",
    "    print(f\"\\nðŸŽ¨ Step 4: Performing pyramid blending ({pyramid_levels} levels)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    blended = blend_with_pyramids(target_image, source_canvas, mask, levels=pyramid_levels)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) * 1000\n",
    "    print(f\"   âœ… Blending complete in {elapsed:.1f}ms\")\n",
    "    \n",
    "    # Visualization\n",
    "    if visualize:\n",
    "        print(\"\\nðŸ“Š Step 5: Creating visualization...\")\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        axes[0, 0].imshow(source_image)\n",
    "        axes[0, 0].set_title('Source (face to swap IN)', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(target_image)\n",
    "        axes[0, 1].set_title('Target (face to swap ONTO)', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(source_canvas)\n",
    "        axes[0, 2].set_title('Aligned Source on Canvas', fontsize=12, fontweight='bold')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        axes[1, 0].imshow(mask, cmap='gray')\n",
    "        axes[1, 0].set_title('Auto-Generated Mask', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(blended)\n",
    "        axes[1, 1].set_title('âœ¨ FINAL RESULT âœ¨', fontsize=12, fontweight='bold', color='green')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        # Side-by-side comparison\n",
    "        comparison = np.hstack([target_image, blended])\n",
    "        axes[1, 2].imshow(comparison)\n",
    "        axes[1, 2].set_title('Before | After', fontsize=12, fontweight='bold')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… Face swap completed successfully!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'result': blended,\n",
    "        'source_canvas': source_canvas,\n",
    "        'mask': mask,\n",
    "        'scale': transform['scale'],\n",
    "        'rotation_degrees': np.degrees(transform['rotation']),\n",
    "        'blending_time_ms': elapsed\n",
    "    }\n",
    "\n",
    "print(\"âœ… Main pipeline function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Upload Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image(prompt=\"Upload an image\"):\n",
    "    \"\"\"\n",
    "    Upload image - works in both Google Colab and local Jupyter.\n",
    "    Returns: (image_array, filename)\n",
    "    \"\"\"\n",
    "    if COLAB_AVAILABLE:\n",
    "        # Google Colab upload\n",
    "        print(f\"ðŸ“¤ {prompt}\")\n",
    "        print(\"   Click 'Choose Files' button below...\\n\")\n",
    "        \n",
    "        uploaded = colab_files.upload()\n",
    "        \n",
    "        if not uploaded:\n",
    "            print(\"âŒ No file uploaded!\")\n",
    "            return None, None\n",
    "        \n",
    "        filename = list(uploaded.keys())[0]\n",
    "        image_bytes = uploaded[filename]\n",
    "        image = np.array(Image.open(io.BytesIO(image_bytes)))\n",
    "        \n",
    "    else:\n",
    "        # Local Jupyter upload using ipywidgets\n",
    "        from ipywidgets import FileUpload\n",
    "        from IPython.display import display, clear_output\n",
    "        \n",
    "        print(f\"ðŸ“¤ {prompt}\")\n",
    "        print(\"   Click 'Upload' button below to select a file...\\n\")\n",
    "        \n",
    "        uploader = FileUpload(accept='image/*', multiple=False)\n",
    "        display(uploader)\n",
    "        \n",
    "        # Wait for upload\n",
    "        import time\n",
    "        while len(uploader.value) == 0:\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        clear_output()\n",
    "        \n",
    "        filename = list(uploader.value.keys())[0]\n",
    "        image_bytes = uploader.value[filename]['content']\n",
    "        image = np.array(Image.open(io.BytesIO(image_bytes)))\n",
    "    \n",
    "    # Convert to RGB if needed\n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 4:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "    \n",
    "    print(f\"âœ… Loaded: {filename}\")\n",
    "    print(f\"   Shape: {image.shape}\\n\")\n",
    "    \n",
    "    return image, filename\n",
    "\n",
    "print(\"âœ… Image upload helper function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Source Image\n",
    "\n",
    "Run the cell below to upload the **source image** (face to swap IN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload source image\n",
    "source_image, source_filename = upload_image(\"Upload SOURCE image (face to swap IN)\")\n",
    "\n",
    "if source_image is not None:\n",
    "    # Display preview\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(source_image)\n",
    "    plt.title(f'Source: {source_filename}', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Target Image\n",
    "\n",
    "Run the cell below to upload the **target image** (face to swap ONTO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload target image\n",
    "target_image, target_filename = upload_image(\"Upload TARGET image (face to swap ONTO)\")\n",
    "\n",
    "if target_image is not None:\n",
    "    # Display preview\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(target_image)\n",
    "    plt.title(f'Target: {target_filename}', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preview Both Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display both images side by side\n",
    "if 'source_image' in locals() and source_image is not None and 'target_image' in locals() and target_image is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].imshow(source_image)\n",
    "    axes[0].set_title(f'Source: {source_filename}', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(target_image)\n",
    "    axes[1].set_title(f'Target: {target_filename}', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Both images loaded successfully!\")\n",
    "    print(\"   Ready to perform face swap.\")\n",
    "else:\n",
    "    print(\"âŒ Please upload both source and target images first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Face Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the face swap\n",
    "if 'source_image' in locals() and source_image is not None and 'target_image' in locals() and target_image is not None:\n",
    "    result = auto_face_swap(\n",
    "        source_image=source_image,\n",
    "        target_image=target_image,\n",
    "        visualize=True,\n",
    "        pyramid_levels=6\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\nðŸ“Š Results Summary:\")\n",
    "        print(f\"   Scale: {result['scale']:.2f}x\")\n",
    "        print(f\"   Rotation: {result['rotation_degrees']:.1f}Â°\")\n",
    "        print(f\"   Blending time: {result['blending_time_ms']:.1f}ms\")\n",
    "        \n",
    "        # Save results\n",
    "        print(\"\\nðŸ’¾ Saving results...\")\n",
    "        cv2.imwrite('ml_face_swap_result.png', cv2.cvtColor(result['result'], cv2.COLOR_RGB2BGR))\n",
    "        cv2.imwrite('ml_face_swap_mask.png', result['mask'])\n",
    "        cv2.imwrite('ml_face_swap_canvas.png', cv2.cvtColor(result['source_canvas'], cv2.COLOR_RGB2BGR))\n",
    "        print(\"âœ… Results saved:\")\n",
    "        print(\"   - ml_face_swap_result.png\")\n",
    "        print(\"   - ml_face_swap_mask.png\")\n",
    "        print(\"   - ml_face_swap_canvas.png\")\n",
    "else:\n",
    "    print(\"âŒ Please upload both source and target images first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "### Features:\n",
    "- **Interactive File Upload** - Works in both Google Colab and local Jupyter\n",
    "- **MediaPipe AI Detection** - 468 facial landmarks\n",
    "- **Automatic Alignment** - Feature-based scale and rotation\n",
    "- **Auto Mask Generation** - From face contours\n",
    "- **Pyramid Blending** - Seamless 6-level blending\n",
    "\n",
    "### To try again:\n",
    "Simply re-run the upload cells (Step 1 & 2) to select different images!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}